{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpysNmYhzGMvwuoAXMQkIq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nabinkumarshaw/chatbot_via_colab/blob/main/chatbot_using_nltk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kcv4-EDUnYBn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('/content/data.txt', 'r',errors ='ignore')\n",
        "raw_doc =f.read()"
      ],
      "metadata": {
        "id": "wgt5_rGwp-g3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(nltk.data.path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dM-eDmdmqmm6",
        "outputId": "8c77089f-d639-4573-c8d9-e3e3ea8a2d06"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/root/nltk_data', '/usr/nltk_data', '/usr/share/nltk_data', '/usr/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_doc =raw_doc.lower()\n",
        "nltk.download('punkt_tab', force=True)\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLkqC6RXq3YY",
        "outputId": "789e9915-f433-4bcc-df5f-7c565f594f1c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_tokens = nltk.sent_tokenize(raw_doc)\n",
        "word_tokens = nltk.word_tokenize(raw_doc)"
      ],
      "metadata": {
        "id": "Br5HsyhavTQQ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_tokens[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofxCBmmwv3JX",
        "outputId": "6c31c9d6-4cf7-46f1-8d4c-9d0419372008"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n\\nwikipediathe free encyclopedia\\nsearch wikipedia\\nsearch\\ndonate\\ncreate account\\nlog in\\n\\ncontents hide\\n(top)\\nbackground\\ndevelopment\\napplication\\n\\nmessaging apps\\nas part of company apps and websites\\nchatbot sequences\\ncompany internal platforms\\ncustomer service\\nhealthcare\\npolitics\\ngovernment\\ntoys\\nmalicious use\\ndata security\\nlimitations of chatbots\\nimpact on jobs\\nimpact on the environment\\nsee also\\nreferences\\nfurther reading\\nexternal links\\nchatbot\\n\\narticle\\ntalk\\nread\\nedit\\nview history\\n\\ntools\\nappearance hide\\ntext\\n\\nsmall\\n\\nstandard\\n\\nlarge\\nwidth\\n\\nstandard\\n\\nwide\\ncolor (beta)\\n\\nautomatic\\n\\nlight\\n\\ndark\\nfrom wikipedia, the free encyclopedia\\nfor the bot-creation software, see chatbot.',\n",
              " 'for bots on internet relay chat, see irc bot.',\n",
              " 'parts of this article (those related to everything, particularly sections after the intro) need to be updated.',\n",
              " 'the reason given is: this article is using citations from 1970 and virtually all claims about conversational capabilities are at least ten years out of date (for example the turing test was arguably made obsolete years ago by transformer models).',\n",
              " 'please help update this article to reflect recent events or newly available information.']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d39IWc2wURa",
        "outputId": "15e20392-dec7-487a-bacb-903501429dde"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['wikipediathe', 'free', 'encyclopedia', 'search', 'wikipedia']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "def LemTokens(tokens):\n",
        "  return [lemmer.lemmatize(token) for token in tokens]\n",
        "remove_punc_dict = dict((ord(punct),None) for punct in string.punctuation)\n",
        "def LemNormalize(text):\n",
        "  return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punc_dict)))"
      ],
      "metadata": {
        "id": "RKxQUMV8wc00"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "greet_inputs = (\"hello\", \"hi\", \"whassup\",\"how are you?\")\n",
        "greet_responses = ('hi','Hey','Hey There!','There there!!')\n",
        "def greet(sentence):\n",
        "  for word in sentence.split():\n",
        "    if word.lower() in greet_inputs:\n",
        "      return random.choice(greet_responses)"
      ],
      "metadata": {
        "id": "TAimKapexbgp"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "XCUal8P9ySXB"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def response(user_response):\n",
        "  robo1_response = ''\n",
        "  TfidfVec =TfidfVectorizer(tokenizer= LemNormalize, stop_words= 'english')\n",
        "  tfidf =TfidfVec.fit_transform(sentence_tokens)\n",
        "  vals = cosine_similarity(tfidf[-1],tfidf)\n",
        "  idx = vals.argsort()[0][-2]\n",
        "  flat =vals.flatten()\n",
        "  flat.sort()\n",
        "  req_tfidf = flat[-2]\n",
        "  if(req_tfidf == 0):\n",
        "    robo1_response = robo1_response + \"I am sorry. Unable to understand you!\"\n",
        "    return robo1_response\n",
        "  else:\n",
        "    robo1_response =robo1_response + sentence_tokens[idx]\n",
        "    return robo1_response"
      ],
      "metadata": {
        "id": "ZMA8YT-ay2-k"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the chatflow"
      ],
      "metadata": {
        "id": "rDkN2rxP0xTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flag = True\n",
        "print('Hello! I am the learning Bot. start typing your text after greeting to talk to me. For ending convo type bye!')\n",
        "while(flag == True):\n",
        "  user_response = input()\n",
        "  user_response = user_response.lower()\n",
        "  if(user_response !='bye'):\n",
        "    if(user_response == 'thank you' or user_response == 'thanks'):\n",
        "      flag=False\n",
        "      print('Bot: You are welcome..')\n",
        "    else:\n",
        "      if(greet(user_response) != None):\n",
        "        print('Bot' + greet(user_response))\n",
        "      else:\n",
        "        sentence_tokens.append(user_response)\n",
        "        word_tokens = word_tokens + nltk.word_tokenize(user_response)\n",
        "        final_words = list(set(word_tokens))\n",
        "        print('Bot: ',end='')\n",
        "        print(response(user_response))\n",
        "        sentence_tokens.remove(user_response)\n",
        "  else:\n",
        "    flag = False\n",
        "    print('Bot: GoodBye!')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hnnAtWe09ZS",
        "outputId": "838b7685-2ecf-4721-d5a3-53ad62e7e9ae"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I am the learning Bot. start typing your text after greeting to talk to me. For ending convo type bye!\n",
            "hi\n",
            "BotHey There!\n",
            "can you tell me about turing test\n",
            "Bot: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chatbot competitions focus on the turing test or more specific goals.\n",
            "nabin\n",
            "Bot: I am sorry. Unable to understand you!\n",
            "turibng test\n",
            "Bot: chatbot competitions focus on the turing test or more specific goals.\n",
            "tur test\n",
            "Bot: chatbot competitions focus on the turing test or more specific goals.\n",
            "tur'\n",
            "Bot: I am sorry. Unable to understand you!\n",
            "tur\n",
            "Bot: I am sorry. Unable to understand you!\n",
            "test\n",
            "Bot: chatbot competitions focus on the turing test or more specific goals.\n",
            "bye\n",
            "Bot: GoodBye!\n"
          ]
        }
      ]
    }
  ]
}